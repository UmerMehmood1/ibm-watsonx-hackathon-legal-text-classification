{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Prompt Notebook with Chat - Prompt Lab Notebook v1.1.0\nThis notebook contains steps and code to demonstrate inferencing of prompts\ngenerated in Prompt Lab in watsonx.ai with a chat format. It introduces Python API commands\nfor authentication using API key and prompt inferencing using WML API.\n\n**Note:** Notebook code generated using Prompt Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Prompt Lab as a notebook.</a>\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n## Notebook goals\nThe learning goals of this notebook are:\n\n* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n* Defining parameters of the Model object\n* Using the Model object to generate response using the defined model id, parameters and the prompt input\n\n# Setup"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install --upgrade 'chromadb==0.3.26' 'pydantic==1.10.0' sentence-transformers\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import os\nimport getpass\n\ndef get_credentials():\n\treturn {\n\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n\t\t\"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n\t}\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Inferencing\nThis cell demonstrated how we can use the model object as well as the created access token\nto pair it with parameters and input string to obtain\nthe response from the the selected foundation model.\n\n## Defining the model id\nWe need to specify model id that will be used for inferencing:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_id = \"ibm/granite-13b-chat-v2\"\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Defining the model parameters\nWe need to provide a set of model parameters that will influence the\nresult:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "parameters = {\n    \"decoding_method\": \"greedy\",\n    \"max_new_tokens\": 8191,\n    \"repetition_penalty\": 1.05\n}"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Defining the project id or space id\nThe API requires project id or space id that provides the context for the call. We will obtain\nthe id from the project or space in which this notebook runs:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Defining the Model object\nWe need to define the Model object using the properties we defined so far:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watsonx_ai.foundation_models import Model\n\nmodel = Model(\n\tmodel_id = model_id,\n\tparams = parameters,\n\tcredentials = get_credentials(),\n\tproject_id = project_id,\n\tspace_id = space_id\n\t)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Defining the vector index\nInitialize the vector index to query when chatting with the model."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watsonx_ai.client import APIClient\n\nwml_credentials = get_credentials()\nclient = APIClient(credentials=wml_credentials, project_id=project_id, space_id=space_id)\n\nvector_index_id = \"1af28755-656b-40db-94f1-b9c74a8509fe\"\nvector_index_details = client.data_assets.get_details(vector_index_id)\nvector_index_properties = vector_index_details[\"entity\"][\"vector_index\"]\n\nfrom ibm_watsonx_ai.foundation_models.embeddings.sentence_transformer_embeddings import SentenceTransformerEmbeddings\n\nemb = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n\nimport subprocess\nimport gzip\nimport json\nimport chromadb\nimport random\nimport string\n\ndef hydrate_chromadb():\n    data = client.data_assets.get_content(vector_index_id)\n    content = gzip.decompress(data)\n    stringified_vectors = str(content, \"utf-8\")\n    vectors = json.loads(stringified_vectors)\n\n    chroma_client = chromadb.Client()\n\n    # make sure collection is empty if it already existed\n    collection_name = \"my_collection\"\n    try:\n        collection = chroma_client.delete_collection(name=collection_name)\n    except:\n        print(\"Collection didn't exist - nothing to do.\")\n    collection = chroma_client.create_collection(name=collection_name)\n\n    vector_embeddings = []\n    vector_documents = []\n    vector_metadatas = []\n    vector_ids = []\n\n    for vector in vectors:\n        vector_embeddings.append(vector[\"embedding\"])\n        vector_documents.append(vector[\"content\"])\n        metadata = vector[\"metadata\"]\n        lines = metadata[\"loc\"][\"lines\"]\n        clean_metadata = {}\n        clean_metadata[\"asset_id\"] = metadata[\"asset_id\"]\n        clean_metadata[\"asset_name\"] = metadata[\"asset_name\"]\n        clean_metadata[\"url\"] = metadata[\"url\"]\n        clean_metadata[\"from\"] = lines[\"from\"]\n        clean_metadata[\"to\"] = lines[\"to\"]\n        vector_metadatas.append(clean_metadata)\n        asset_id = vector[\"metadata\"][\"asset_id\"]\n        random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n        id = \"{}:{}-{}-{}\".format(asset_id, lines[\"from\"], lines[\"to\"], random_string)\n        vector_ids.append(id)\n\n    collection.add(\n        embeddings=vector_embeddings,\n        documents=vector_documents,\n        metadatas=vector_metadatas,\n        ids=vector_ids\n    )\n    return collection\n\nchroma_collection = hydrate_chromadb()\n\ndef proximity_search( question ):\n    query_vectors = emb.embed_query(question)\n    query_result = chroma_collection.query(\n        query_embeddings=query_vectors,\n        n_results=vector_index_properties[\"settings\"][\"top_k\"],\n        include=[\"documents\", \"metadatas\", \"distances\"]\n    )\n\n    documents = list(reversed(query_result[\"documents\"][0]))\n\n    return \"\\n\".join(documents)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Defining the inferencing input for chat\nFoundation models supporting chat accept a system prompt that instructs the model on how to conduct the dialog. They also accept previous questions and answers to give additional context when inferencing. Each model has it's own string format for constructing the input.\n\nLet us provide the input we got from the Prompt Lab and format it for the selected model:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "prompt_input = \"\"\"<|system|>\nSystem Prompt: AI Legal Text Classification\nPurpose:\nYou are an AI model designed to classify legal texts into predefined categories to assist legal professionals in document management and retrieval.\n\nInstructions:\nInput Types:\nLegal documents such as contracts, case law, statutes, legal briefs, etc.\nText snippets or excerpts from larger documents.\nCategories for Classification:\nContracts\nLitigation\nLegislation\nRegulatory Compliance\nLegal Opinions\nCase Law\nIntellectual Property\nFamily Law\nCriminal Law\nCorporate Law\nOutput Requirements:\nFormat:\nRespond using bolded headings with properly indented subsections for the output.\n\n1. Summarization\nProvide a brief summary of the input text based on its legal context.\nThis should explain the core content in concise terms.\n2. Classification\nCategory: The most appropriate legal category for the text.\nConfidence Score: Indicate the certainty of the classification (0-100%).\n3. Reasoning\nExplanation: Why the text is classified into the selected category.\nHandling of Ambiguity: If the text contains ambiguous or mixed content, describe how the final classification decision was reached.\nAdditional Context:\nIf the input text contains ambiguous or mixed content, discuss any contributing factors in the final classification decision.\nEnsure the reasoning clearly explains the connection between the text and the chosen category.\nExample Input:\n\"Terms and conditions for software usage.\"\n\nExpected Output:\n1. Summarization\nThis text outlines the terms and conditions for the usage of a software product, likely specifying the rights and obligations of the users.\n2. Classification\nCategory: Contracts\nConfidence Score: 95%\n3. Reasoning\nExplanation: The text refers to a legal agreement between parties, which is typically covered under contracts. Given the nature of the text involving software usage terms, it strongly aligns with the Contracts category.\nHandling of Ambiguity: The content is straightforward and does not present mixed or ambiguous elements.\n<|user|>\nclassify this document\n<|assistant|>\n1. Summarization: This text presents a legal principle regarding the awarding of costs in court cases, specifically mentioning that costs generally follow the event and are awarded on a party and party basis.\n2. Classification: Case Law\n3. Reasoning: The text discusses a specific court case, Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Ltd (No 2) [2002] FCA 224 ; (2002) 190 ALR 121, where the court applied this legal principle. Therefore, the document can be classified as Case Law.\n\"\"\"\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Execution\nLet us now use the defined Model object, pair it with the input, and generate the response to your question:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "question = input(\"Question: \")\ngrounding = proximity_search(question)\nformattedQuestion = f\"\"\"<|user|>\n[Document]\n{grounding}\n[End]\n{question}\n<|assistant|>\n\"\"\"\nprompt = f\"\"\"{prompt_input}{formattedQuestion}\"\"\"\ngenerated_response = model.generate_text(prompt=prompt.replace(\"__grounding__\", grounding), guardrails=False)\nprint(f\"AI: {generated_response}\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Next steps\nYou successfully completed this notebook! You learned how to use\nwatsonx.ai inferencing SDK to generate response from the foundation model\nbased on the provided input, model id and model parameters. Check out the\nofficial watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2023 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}